{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import cvzone\n",
    "import os\n",
    "import numpy as np\n",
    "import sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement yolo to video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name=='nt':\n",
    "    !pip install wget\n",
    "    import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sort.py'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "wget.download(\"https://raw.githubusercontent.com/abewley/sort/master/sort.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \".\\Video\\TrafficFlow.mp4\"\n",
    "image_path = './Image/imageFilter.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('./model/yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.11.8 torch-2.1.2+cpu CPU (Intel Core(TM) i5-7200U 2.50GHz)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'model\\yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
      "Collecting onnx>=1.12.0\n",
      "  Downloading onnx-1.16.0-cp311-cp311-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in f:\\python\\everydaycoding\\computervision\\objectdetection\\yolo\\carcount\\.venv\\lib\\site-packages (from onnx>=1.12.0) (1.26.3)\n",
      "Collecting protobuf>=3.20.2 (from onnx>=1.12.0)\n",
      "  Downloading protobuf-5.26.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Downloading onnx-1.16.0-cp311-cp311-win_amd64.whl (14.4 MB)\n",
      "   ---------------------------------------- 14.4/14.4 MB 195.5 kB/s eta 0:00:00\n",
      "Downloading protobuf-5.26.1-cp310-abi3-win_amd64.whl (420 kB)\n",
      "   -------------------------------------- 420.9/420.9 kB 164.3 kB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, onnx\n",
      "Successfully installed onnx-1.16.0 protobuf-5.26.1\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 102.0s, installed 1 package: ['onnx>=1.12.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 108.0s, saved as 'model\\yolov8n.onnx' (12.2 MB)\n",
      "\n",
      "Export complete (112.2s)\n",
      "Results saved to \u001b[1mF:\\python\\EveryDayCoding\\ComputerVision\\ObjectDetection\\YOLO\\CarCount\\model\u001b[0m\n",
      "Predict:         yolo predict task=detect model=model\\yolov8n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=model\\yolov8n.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model\\\\yolov8n.onnx'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('./model/yolov8n.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 1 htyellow, 1231.1ms\n",
      "Speed: 16.9ms preprocess, 1231.1ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.73\n",
      "\n",
      "0: 320x640 1 htyellow, 995.3ms\n",
      "Speed: 4.0ms preprocess, 995.3ms inference, 0.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.76\n",
      "\n",
      "0: 320x640 1 htyellow, 709.5ms\n",
      "Speed: 7.5ms preprocess, 709.5ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.78\n",
      "\n",
      "0: 320x640 1 htyellow, 904.4ms\n",
      "Speed: 6.0ms preprocess, 904.4ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.82\n",
      "\n",
      "0: 320x640 1 htyellow, 940.2ms\n",
      "Speed: 4.0ms preprocess, 940.2ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.83\n",
      "\n",
      "0: 320x640 1 htyellow, 713.4ms\n",
      "Speed: 4.0ms preprocess, 713.4ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.83\n",
      "\n",
      "0: 320x640 1 htyellow, 1 htwhite, 936.1ms\n",
      "Speed: 4.0ms preprocess, 936.1ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.83\n",
      "0.33\n",
      "\n",
      "0: 320x640 1 htyellow, 1 htwhite, 889.6ms\n",
      "Speed: 5.3ms preprocess, 889.6ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.8\n",
      "0.35\n",
      "\n",
      "0: 320x640 1 htyellow, 1 htwhite, 1301.2ms\n",
      "Speed: 2.0ms preprocess, 1301.2ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.81\n",
      "0.58\n",
      "\n",
      "0: 320x640 1 htyellow, 1 htwhite, 1228.7ms\n",
      "Speed: 6.0ms preprocess, 1228.7ms inference, 0.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "0.83\n",
      "0.72\n"
     ]
    }
   ],
   "source": [
    "wCam, hCam = 640, 480\n",
    "\n",
    "image_overlay = cv2.imread(image_path)\n",
    "# image_overlay = cv2.resize(image_overlay, (wCam, hCam))\n",
    "\n",
    "capture = cv2.VideoCapture(video_path)\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, wCam)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, hCam)\n",
    "pTime = 0\n",
    "object_name = [\"car\", \"truck\", \"bus\", \"motorbike\"]\n",
    "\n",
    "tracker = sort.Sort(max_age=20, min_hits=3, iou_threshold=0.2)\n",
    "totalCount = []\n",
    "\n",
    "line = (100, hCam//2, 600, hCam//2)\n",
    "while (capture.isOpened()):\n",
    "    ret, frame = capture.read() \n",
    "    image_overlay = cv2.resize(image_overlay, (frame.shape[1], frame.shape[0]))\n",
    "    frame_overlay = cv2.bitwise_and(image_overlay, frame)\n",
    "    results = model(frame_overlay)[0]\n",
    "    object_info_detected = np.empty((0, 5))\n",
    "    \n",
    "    if ret == True: \n",
    "        # yolo\n",
    "        for box in results.boxes:\n",
    "            class_name = results.names[box.cls[0].item()] # Class name\n",
    "            cords = box.xyxy[0].tolist()\n",
    "            x1, y1, x2, y2 = (round(x) for x in cords)\n",
    "            x, y, w, h = x1, y1, x2 - x1, y2 - y1    \n",
    "            confidence = round(box.conf[0].item(), 2)\n",
    "            print(confidence)\n",
    "            # cv2.rectangle(frame,(x1,y1),(x2,y2),(255,0,255),3)\n",
    "            \n",
    "            # cvzone.cornerRect(\n",
    "            #                     frame,  # The image to draw on\n",
    "            #                     (x, y, w, h ),  # The position and dimensions of the rectangle (x, y, width, height)\n",
    "            #                     l=30,  # Length of the corner edges\n",
    "            #                     t=2,  # Thickness of the corner edges\n",
    "            #                     rt=1,  # Thickness of the rectangle\n",
    "            #                     colorR=(255, 0, 255),  # Color of the rectangle\n",
    "            #                     colorC=(0, 255, 0)  # Color of the corner edges\n",
    "            #                 )\n",
    "            \n",
    "            \n",
    "            # cvzone.putTextRect(\n",
    "            #                     frame, f\"{confidence} {class_name}\", (max(0, x), max(30, y)),  # Image and starting position of the rectangle\n",
    "            #                     scale=1, thickness=1,  # Font scale and thickness\n",
    "            #                     colorT=(0, 0, 255), colorR=(255, 100, 0),  # Text color and Rectangle color\n",
    "            #                     font=cv2.FONT_HERSHEY_PLAIN,  # Font type\n",
    "            #                     offset=1,  # Offset of text inside the rectangle\n",
    "            #                     border=1, colorB=(0, 5, 0)  # Border thickness and color\n",
    "            #                 )\n",
    "            \n",
    "            if class_name in object_name and confidence > 0.5:\n",
    "                object_info_array = np.array([x1, y1, x2, y2, confidence])\n",
    "                object_info_detected = np.vstack((object_info_detected, object_info_array))\n",
    "        \n",
    "        resultTracker = tracker.update(object_info_detected)\n",
    "        cv2.line(frame, (line[0], line[1]), (line[2], line[3]), (0, 0, 255), 3)\n",
    "        \n",
    "        for result_track in resultTracker:\n",
    "            x1, y1, x2, y2, id = result_track\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            x, y, w, h = x1, y1, x2 - x1, y2 - y1  \n",
    "            \n",
    "            cvzone.cornerRect(\n",
    "                    frame,  # The image to draw on\n",
    "                    (x, y, w, h ),  # The position and dimensions of the rectangle (x, y, width, height)\n",
    "                    l=30,  # Length of the corner edges\n",
    "                    t=2,  # Thickness of the corner edges\n",
    "                    rt=1,  # Thickness of the rectangle\n",
    "                    colorR=(255, 0, 255),  # Color of the rectangle\n",
    "                    colorC=(0, 255, 0)  # Color of the corner edges\n",
    "                )\n",
    "            \n",
    "            cvzone.putTextRect(\n",
    "                                frame, f\"{id}\", (max(0, x), max(30, y)),  # Image and starting position of the rectangle\n",
    "                                scale=1, thickness=1,  # Font scale and thickness\n",
    "                                colorT=(0, 0, 255), colorR=(255, 100, 0),  # Text color and Rectangle color\n",
    "                                font=cv2.FONT_HERSHEY_PLAIN,  # Font type\n",
    "                                offset=1,  # Offset of text inside the rectangle\n",
    "                                border=1, colorB=(0, 5, 0)  # Border thickness and color\n",
    "                            )\n",
    "            \n",
    "            centerX, centerY = x1 + w//2, y1 + h//2\n",
    "            \n",
    "            cv2.circle(frame, (centerX, centerY), 4, (0, 225, 255), cv2.FILLED)\n",
    "            \n",
    "            if line[0] < centerX < line[2] and line[1] - 5 < centerY < line[3]:\n",
    "                if totalCount.count(id) == 0:\n",
    "                    totalCount.append(id)\n",
    "                    cv2.line(frame, (line[0], line[1]), (line[2], line[3]), (255, 0, 0), 3)\n",
    "            \n",
    "            cvzone.putTextRect(\n",
    "                    frame, f\"Count viachle passed : {len(totalCount)}\", (int(wCam//6), int(hCam//10)),  # Image and starting position of the rectangle\n",
    "                    scale=1, thickness=1,  # Font scale and thickness\n",
    "                    colorT=(0, 0, 255), colorR=(255, 100, 0),  # Text color and Rectangle color\n",
    "                    font=cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "                    offset=2,  # Offset of text inside the rectangle\n",
    "                    border=1, colorB=(0, 5, 0)  # Border thickness and color\n",
    "                )\n",
    "            \n",
    "        # frame = cv2.resize(frame, (640, 640), \n",
    "        #        interpolation = cv2.INTER_LINEAR)\n",
    "        cv2.imshow('Frame', frame) \n",
    "        \n",
    "        if cv2.waitKey(25) in (ord('q'), ord('Q')):\n",
    "            break\n",
    "    else: \n",
    "        break\n",
    "  \n",
    "# After the loop release the capture object \n",
    "capture.release() \n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object_Detection:\n",
    "    def __init__(self, model:str, width:int, height:int, object_name:typing.List[str] = [\"car\", \"truck\", \"bus\", \"motorbike\"]):\n",
    "        self.object_name = object_name\n",
    "        self.model = YOLO(model)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.object_name = object_name\n",
    "        self.frame = None\n",
    "        \n",
    "\n",
    "    # set image\n",
    "    def set_image(self, image_path:str):\n",
    "        image_overlay = cv2.imread(image_path)\n",
    "        image_overlay = cv2.resize(image_overlay, (self.width, self.height))\n",
    "        return image_overlay\n",
    "    \n",
    "    \n",
    "    # draw reactangle\n",
    "    def set_reactangle(self, \n",
    "                       x1, y1, x2, y2, \n",
    "                       length, \n",
    "                       thickness_edges, \n",
    "                       thickness_rectangle, \n",
    "                       color_rectangle:typing.Tuple[int:\"Blue\", int:\"Green\", int:\"Red\"], \n",
    "                       color_corners:typing.Tuple[int:\"Blue\", int:\"Green\", int:\"Red\"]):\n",
    "        \n",
    "        \"\"\"\n",
    "        description: Draw reactangle on frame\n",
    "                x1, y1, x2, y2: position and dimensions of the rectangle \n",
    "                length: length of the edges\n",
    "                thickness_edges: thickness of the edges\n",
    "                thickness_rectangle: thickness of the rectangle\n",
    "                color_rectangle: color of the rectangle\n",
    "                color_corners: color of the corners\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        cvzone.cornerRect(\n",
    "                self.frame, \n",
    "                (x1, y1, x2 - x1, y2 - y1), \n",
    "                l=length, \n",
    "                t=thickness_edges, \n",
    "                rt=thickness_rectangle, \n",
    "                colorR=color_rectangle, \n",
    "                colorC=color_corners \n",
    "            )\n",
    "        \n",
    "    \n",
    "    # draw text\n",
    "    def set_Text(self, \n",
    "                 text:str, \n",
    "                 font_scale:int, \n",
    "                 thickness:int,\n",
    "                 x:int, y:int, \n",
    "                 offset:int, \n",
    "                 border:int,\n",
    "                 color_text:typing.Tuple[int:\"Blue\", int:\"Green\", int:\"Red\"],\n",
    "                 color_rectangle:typing.Tuple[int:\"Blue\", int:\"Green\", int:\"Red\"],\n",
    "                 color_border:typing.Tuple[int:\"Blue\", int:\"Green\", int:\"Red\"]\n",
    "                 ):\n",
    "        \n",
    "        \"\"\"\n",
    "        description: Draw text on frame\n",
    "                text: text\n",
    "                font_scale: font scale\n",
    "                thickness: thickness\n",
    "                x, y: position\n",
    "                offset: offset\n",
    "                border: border\n",
    "                color_text: color of the text\n",
    "                color_rectangle: color of the rectangle\n",
    "                color_border: color of the border\n",
    "        \"\"\"\n",
    "\n",
    "        cvzone.putTextRect(\n",
    "                self.frame, text, (x, y), \n",
    "                scale=font_scale, thickness=thickness,  \n",
    "                colorT=color_text, colorR=color_rectangle,  \n",
    "                font=cv2.FONT_HERSHEY_SCRIPT_COMPLEX, \n",
    "                offset=offset,  \n",
    "                border=border, colorB=color_border \n",
    "            )\n",
    "        \n",
    "\n",
    "    # get inference\n",
    "    def _get_inference(self, image):\n",
    "        results = self.model(image)[0]\n",
    "        return results\n",
    "    \n",
    "    \n",
    "    # get box\n",
    "    def get_box(self, results):\n",
    "        for box in results.boxes:\n",
    "            class_name = results.names[box.cls[0].item()] # Class name\n",
    "            cords = box.xyxy[0].tolist()\n",
    "            x1, y1, x2, y2 = (round(x) for x in cords)\n",
    "            # x, y, w, h = x1, y1, x2 - x1, y2 - y1    \n",
    "            confidence = round(box.conf[0].item(), 2)\n",
    "        \n",
    "            self.set_reactangle(\n",
    "                                x1, y1, x2, y2, \n",
    "                                length=30, \n",
    "                                thickness_edges=2, \n",
    "                                thickness_rectangle=1, \n",
    "                                color_rectangle=(255, 0, 255), \n",
    "                                color_corners=(0, 255, 0) \n",
    "                                )\n",
    "    \n",
    "            \n",
    "    def run_video(self, video_path:str):\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "        capture.set(3, self.width)\n",
    "        capture.set(4, self.height)\n",
    "        \n",
    "        while (capture.isOpened()):\n",
    "            ret, frame = capture.read() \n",
    "            self.frame = frame\n",
    "            results = self._get_inference(frame)\n",
    "            # results = model(frame)[0]\n",
    "               \n",
    "            if ret == True:\n",
    "                self.get_box(results)\n",
    "                \n",
    "                if cv2.waitKey(25) in (ord('q'), ord('Q')):\n",
    "                    break\n",
    "            else: \n",
    "                break\n",
    "            \n",
    "            cv2.imshow('Frame', frame) \n",
    "        \n",
    "        capture.release() \n",
    "        cv2.destroyAllWindows()\n",
    "            \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the OOP havent done yet, just look the scratch code for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 20 cars, 1 bus, 2 trucks, 249.5ms\n",
      "Speed: 0.0ms preprocess, 249.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 1 bus, 2 trucks, 223.1ms\n",
      "Speed: 4.0ms preprocess, 223.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 138.2ms\n",
      "Speed: 3.4ms preprocess, 138.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 1 truck, 146.9ms\n",
      "Speed: 0.0ms preprocess, 146.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 1 truck, 135.9ms\n",
      "Speed: 0.0ms preprocess, 135.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 129.6ms\n",
      "Speed: 2.0ms preprocess, 129.6ms inference, 13.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 149.7ms\n",
      "Speed: 2.0ms preprocess, 149.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 137.8ms\n",
      "Speed: 2.0ms preprocess, 137.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 145.8ms\n",
      "Speed: 0.0ms preprocess, 145.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 165.9ms\n",
      "Speed: 0.0ms preprocess, 165.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 137.6ms\n",
      "Speed: 0.0ms preprocess, 137.6ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 1 truck, 148.6ms\n",
      "Speed: 2.0ms preprocess, 148.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 cars, 1 bus, 1 truck, 150.3ms\n",
      "Speed: 0.0ms preprocess, 150.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 1 truck, 152.4ms\n",
      "Speed: 0.0ms preprocess, 152.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 1 truck, 143.2ms\n",
      "Speed: 0.0ms preprocess, 143.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 cars, 1 bus, 1 truck, 152.3ms\n",
      "Speed: 5.5ms preprocess, 152.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 1 truck, 159.6ms\n",
      "Speed: 2.0ms preprocess, 159.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 1 truck, 181.7ms\n",
      "Speed: 0.0ms preprocess, 181.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 cars, 1 bus, 1 truck, 135.2ms\n",
      "Speed: 3.0ms preprocess, 135.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 1 truck, 151.9ms\n",
      "Speed: 0.0ms preprocess, 151.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 1 bus, 1 truck, 138.9ms\n",
      "Speed: 0.0ms preprocess, 138.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 2 buss, 1 truck, 138.0ms\n",
      "Speed: 0.0ms preprocess, 138.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 cars, 3 buss, 1 truck, 159.2ms\n",
      "Speed: 0.0ms preprocess, 159.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 3 buss, 1 truck, 135.1ms\n",
      "Speed: 5.4ms preprocess, 135.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 2 buss, 1 truck, 135.2ms\n",
      "Speed: 3.0ms preprocess, 135.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 2 buss, 1 truck, 134.9ms\n",
      "Speed: 0.0ms preprocess, 134.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 cars, 2 buss, 1 truck, 148.8ms\n",
      "Speed: 0.0ms preprocess, 148.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 2 buss, 1 truck, 141.7ms\n",
      "Speed: 0.0ms preprocess, 141.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 2 buss, 1 truck, 152.3ms\n",
      "Speed: 0.0ms preprocess, 152.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "OD = Object_Detection('./model/yolov8n.pt', 640, 480, [\"car\", \"truck\", \"bus\", \"motorbike\"])\n",
    "OD.run_video(video_path='.\\Video\\TrafficFlow.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
